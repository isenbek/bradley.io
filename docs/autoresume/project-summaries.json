[
  {
    "name": "Adventures in Computing Portfolio",
    "tagline": "Curiosity-driven exploration spanning distributed systems, AI/ML frameworks, and experimental computing",
    "purpose": "A collection of maker projects demonstrating hardware hacking, edge computing, and experimental systems that push technical boundaries. These projects represent active exploration beyond production systems, showcasing creative solutions within constraints and a strong DIY ethos.",
    "technologies": [
      "Python",
      "C",
      "IPv6",
      "Machine Learning",
      "WebSocket",
      "ADS-B",
      "DuckDB",
      "scikit-learn",
      "GitHub API",
      "DeepSeek-Coder-V2",
      "Docker",
      "InspIRCd",
      "Matterbridge",
      "Ollama",
      "Ed25519",
      "Curve25519",
      "FastAPI",
      "DynamoDB",
      "PostgreSQL",
      "Redis",
      "Lark Parser",
      "pytest"
    ],
    "role": "architect",
    "innovations": [
      "Standardized ML pipeline for aviation data with <100ms prediction latency",
      "Biological-inspired code ecosystem applying bacterial behavior to distributed computing",
      "Local LLM integration with Ollama for privacy-preserving AI chat",
      "Lock-free circular buffer task queue with IPv6-native multicast discovery",
      "32-instruction minimal assembly-like language with error-driven evolution",
      "60-node Raspberry Pi distributed investigation cluster with VPN/Tor architecture",
      "Thorium-based TRNG system with statistical validation"
    ],
    "enterprise_relevance": "Demonstrates advanced distributed systems architecture, real-time data processing at scale, security-first design patterns, and production-ready ML pipeline development applicable to enterprise infrastructure",
    "maker_relevance": "Strong DIY ethos with self-hosted infrastructure, salvaged hardware projects, custom protocols, and experimental systems built from scratch using accessible tools and open-source technologies",
    "category": "hybrid",
    "claude_involvement": "Used for research synthesis, framework design, architectural documentation, and integrated as co-worker into development workflow for enhanced productivity",
    "complexity_score": 9,
    "resume_highlight": "Built production-ready aviation ML framework processing 50+ aircraft/second with sub-100ms latency while maintaining active maker practice including 60-node Raspberry Pi cluster and self-improving programming language",
    "project_dir": "bradleyio",
    "scanned_at": "2026-01-15T11:11:31.576039"
  },
  {
    "name": "Clarion API",
    "tagline": "AI-powered newsletter aggregation and digest platform with creator tools",
    "purpose": "A comprehensive REST API platform that aggregates newsletters and RSS feeds, generates AI-powered daily digests, and provides a creator platform for publishing articles. The system handles user management, subscriptions, notifications, and content discovery with multi-provider AI integration.",
    "technologies": [
      "REST API",
      "JWT Authentication",
      "Ollama",
      "Claude AI",
      "Mistral OCR",
      "PostgreSQL",
      "Server-Sent Events",
      "Stripe",
      "PayPal",
      "WebSocket"
    ],
    "role": "architect",
    "innovations": [
      "Multi-provider AI orchestration (local Ollama + cloud Claude/Mistral)",
      "Streaming AI chat with SSE",
      "Social feed sharing workflow",
      "Hybrid creator platform within digest aggregator",
      "Premium AI mode switching"
    ],
    "enterprise_relevance": "Production-grade API with 85 endpoints, role-based access control, multi-tenant billing, webhook integrations, job queue system, and comprehensive audit trails suitable for SaaS deployment",
    "maker_relevance": "Demonstrates full-stack API design with AI integration, background job processing, real-time features, and monetization infrastructure",
    "category": "professional",
    "claude_involvement": "Claude AI integrated as premium cloud provider for high-quality summarization and topic extraction alongside local Ollama models",
    "complexity_score": 9,
    "resume_highlight": "Architected 85-endpoint REST API with multi-provider AI orchestration, streaming chat, creator platform, and subscription billing",
    "project_dir": "clarion",
    "scanned_at": "2026-01-15T11:11:42.249224"
  },
  {
    "name": "Crawwwl",
    "tagline": "Autonomous semantic web crawling and knowledge discovery system with iterative information retrieval",
    "purpose": "A sophisticated pipeline for web content crawling, semantic analysis, and autonomous knowledge discovery. The system uses vector embeddings, AI-driven exploration, and iterative refinement to build comprehensive knowledge maps from web sources, with support for multiple search engines, quality filtering, and semantic relationship identification.",
    "technologies": [
      "Python",
      "Bash",
      "Ollama",
      "Vector Embeddings",
      "BeautifulSoup",
      "curl",
      "lynx"
    ],
    "role": "architect",
    "innovations": [
      "Autonomous iterative knowledge discovery with AI-driven exploration",
      "Multi-modal content processing with semantic seed filtering",
      "Vector-based semantic recovery pipeline with clustering",
      "Browser impersonation rotation for robust web crawling",
      "Quality-scored chunking with configurable overlap parameters"
    ],
    "enterprise_relevance": "Applicable to competitive intelligence, market research, knowledge management systems, and automated content curation at scale with quality filtering and semantic analysis",
    "maker_relevance": "Demonstrates advanced AI/ML integration, distributed systems design, and practical implementation of semantic search and autonomous discovery systems",
    "category": "professional",
    "claude_involvement": "Used for semantic analysis, autonomous knowledge discovery, gap analysis, and query generation in the iterative exploration pipeline",
    "complexity_score": 9,
    "resume_highlight": "Architected autonomous semantic web crawler with vector embeddings, achieving iterative knowledge discovery through AI-driven exploration and multi-engine content aggregation",
    "project_dir": "crawwwl",
    "scanned_at": "2026-01-15T11:11:52.930107"
  },
  {
    "name": "Spondr Aviation AI/ML Framework",
    "tagline": "Real-time aviation intelligence platform with standardized ML pipeline for live ADS-B aircraft data",
    "purpose": "A production-ready framework for rapid development and deployment of aviation AI models using live ADS-B data streams. Enables multiple simultaneous models for aircraft behavior prediction, anomaly detection, and traffic optimization with sub-100ms latency and 50+ aircraft/second throughput.",
    "technologies": [
      "Python",
      "Machine Learning",
      "WebSocket",
      "ADS-B",
      "DuckDB",
      "scikit-learn",
      "Real-time Data Processing",
      "Node.js"
    ],
    "role": "architect",
    "innovations": [
      "Standardized ML pipeline for aviation data with <100ms prediction latency",
      "Multi-model architecture supporting simultaneous real-time inference from single data stream",
      "Template-based framework enabling new model deployment in 5 steps",
      "Production-ready landing predictor achieving 94%+ accuracy with live ADS-B integration"
    ],
    "enterprise_relevance": "Demonstrates scalable real-time ML architecture, production system design, and standardized development workflows applicable to IoT/telemetry processing, predictive maintenance, and operational intelligence platforms",
    "maker_relevance": "Aviation enthusiast project processing live aircraft data with accessible ML techniques, showcasing practical application of data science to real-world flight tracking and prediction",
    "category": "hybrid",
    "claude_involvement": null,
    "complexity_score": 9,
    "resume_highlight": "Architected production ML framework processing 50+ aircraft/second with 94% prediction accuracy and <100ms latency, featuring standardized pipeline enabling rapid model development",
    "project_dir": "dragonfli",
    "scanned_at": "2026-01-15T11:12:05.574823"
  },
  {
    "name": "Flock",
    "tagline": "Mobile-first WASM political CRM with real-time collaboration",
    "purpose": "A high-performance CRM application designed for political field workers that prioritizes mobile-first design, real-time collaboration, and developer velocity. The system uses WebAssembly for business logic, maintains sub-50KB initial bundles, and achieves <3-second time-to-interactive on mobile devices.",
    "technologies": [
      "AssemblyScript",
      "WASM",
      "uWebSockets.js",
      "Preact",
      "Vite",
      "DuckDB",
      "Redis",
      "Tailwind CSS",
      "TypeScript",
      "WebSockets"
    ],
    "role": "architect",
    "innovations": [
      "AssemblyScript WASM modules for contact scoring and validation (3-5KB vs 50-70KB Rust)",
      "uWebSockets.js handling 10,000+ concurrent connections with 201MB memory",
      "Signal-based reactivity with WASM bridge for shared state",
      "Mobile-optimized real-time collaboration with presence awareness"
    ],
    "enterprise_relevance": "Demonstrates expertise in building scalable real-time systems with modern web technologies, performance optimization for mobile devices, and architectural design for collaborative applications",
    "maker_relevance": "Showcases cutting-edge 2025 web technologies including WASM, modern build tools, and performance-first mobile development patterns",
    "category": "professional",
    "claude_involvement": null,
    "complexity_score": 9,
    "resume_highlight": "Architected mobile-first political CRM achieving sub-50KB bundles and <3s time-to-interactive using AssemblyScript WASM, uWebSockets.js for 10K+ concurrent connections, and signal-based reactivity",
    "project_dir": "flock",
    "scanned_at": "2026-01-15T11:12:15.535385"
  },
  {
    "name": "Biological-Inspired Code Ecosystem",
    "tagline": "Self-organizing code ecosystem where GitHub gists behave like living bacteria",
    "purpose": "Creating a distributed computing system that applies biological principles like chemotaxis, quorum sensing, and horizontal gene transfer to enable GitHub gists to self-organize, evolve, and collaborate autonomously. The system uses AI-powered code analysis to enable gists to discover compatible partners, share code fragments, and form functional clusters without central control.",
    "technologies": [
      "GitHub API",
      "DeepSeek-Coder-V2",
      "Linux Seccomp",
      "Docker",
      "Genetic Programming",
      "Unix Pipes",
      "OpenAPI",
      "Python"
    ],
    "role": "architect",
    "innovations": [
      "Applying bacterial behavior patterns to distributed code organization",
      "Using LLM-guided mutations for code evolution",
      "Implementing chemotaxis algorithms for code migration toward optimal environments",
      "Creating membrane-based communication protocols for cross-language gist interaction",
      "Designing quorum sensing mechanisms for collective code activation"
    ],
    "enterprise_relevance": "Demonstrates novel approaches to distributed system architecture, autonomous code optimization, and self-organizing microservices that could revolutionize how enterprise systems adapt and evolve",
    "maker_relevance": "Explores cutting-edge intersection of artificial life, evolutionary computing, and practical code automation using accessible tools like GitHub gists and local AI models",
    "category": "hybrid",
    "claude_involvement": "Research synthesis and framework design for implementing biological computing principles in distributed code systems",
    "complexity_score": 9,
    "resume_highlight": "Architected novel biological-inspired distributed computing framework applying bacterial behaviors to autonomous code evolution",
    "project_dir": "gcombinatr",
    "scanned_at": "2026-01-15T11:12:26.252705"
  },
  {
    "name": "Hotbits TRNG",
    "tagline": "True random number generator extracting cryptographic-quality entropy from hardware timing jitter",
    "purpose": "Transform raw timestamp deltas from hardware events into high-quality random bit streams by identifying and removing periodic/deterministic signals while preserving true quantum entropy. Develop a filtering and extraction pipeline that passes cryptographic randomness tests (dieharder, NIST) for applications requiring unpredictable randomness.",
    "technologies": [
      "Python",
      "FFT",
      "Signal Processing",
      "Cryptography",
      "SHA-256",
      "NIST Statistical Tests",
      "Dieharder",
      "Wavelet Transform",
      "NumPy"
    ],
    "role": "architect",
    "innovations": [
      "Adaptive threshold extraction from nanosecond timing jitter",
      "Multi-stage filtering pipeline removing periodic system noise while preserving entropy",
      "Real-time entropy quality testing framework",
      "Differential encoding and phase extraction methods for bias removal"
    ],
    "enterprise_relevance": "Cryptographic key generation, secure authentication systems, blockchain applications, hardware security modules, and any system requiring provably unpredictable randomness",
    "maker_relevance": "Creative applications like quantum art generation, meditation oracles, serendipity engines, and experimental projects requiring true hardware randomness",
    "category": "hybrid",
    "claude_involvement": null,
    "complexity_score": 8,
    "resume_highlight": "Designed entropy extraction pipeline achieving >95% pass rate on cryptographic randomness tests by removing deterministic signals from hardware timing data",
    "project_dir": "hotbits",
    "scanned_at": "2026-01-15T11:12:36.352416"
  },
  {
    "name": "Lazarus FastAPI Implementation",
    "tagline": "RESTful API and distributed task processing system for document analysis pipeline",
    "purpose": "Design and implement a production-grade FastAPI backend with Celery task queue for the Lazarus document processing system. The API provides job management, session tracking, semantic search, entity extraction, and automated report generation capabilities with WebSocket support for real-time updates.",
    "technologies": [
      "FastAPI",
      "Celery",
      "Redis",
      "PostgreSQL",
      "SQLAlchemy",
      "Alembic",
      "Docker",
      "Kubernetes",
      "WebSockets",
      "Nginx",
      "Pydantic"
    ],
    "role": "architect",
    "innovations": [
      "Distributed document processing pipeline with async task orchestration",
      "Real-time job status tracking via WebSocket connections",
      "Semantic search integration with vector store caching",
      "Automated story generation and timeline analysis as background tasks"
    ],
    "enterprise_relevance": "Demonstrates ability to architect scalable microservices with proper separation of concerns, database migrations, containerization, and production deployment strategies including Kubernetes orchestration",
    "maker_relevance": null,
    "category": "professional",
    "claude_involvement": null,
    "complexity_score": 9,
    "resume_highlight": "Architected comprehensive FastAPI microservices system with Celery distributed task queue, PostgreSQL persistence, Redis caching, WebSocket real-time updates, and full Kubernetes deployment strategy",
    "project_dir": "lazarus",
    "scanned_at": "2026-01-15T11:12:45.628214"
  },
  {
    "name": "LlamaIRC - Comprehensive IRC Infrastructure for AI Communities",
    "tagline": "Modern IRC infrastructure with local LLM integration and multi-platform bridging",
    "purpose": "Designed and documented a complete IRC communication infrastructure optimized for AI communities, combining InspIRCd server architecture with Matterbridge for multi-platform connectivity and local LLM integration via Ollama. The system provides privacy-preserving AI capabilities while maintaining IRC's core strengths of efficiency, customization, and security for technically advanced users seeking complete control over their communication infrastructure.",
    "technologies": [
      "InspIRCd",
      "Matterbridge",
      "Ollama",
      "Ergo",
      "UnrealIRCd",
      "Matrix",
      "Discord",
      "Atheme",
      "PostgreSQL",
      "HAProxy",
      "Prometheus",
      "Grafana",
      "fail2ban",
      "CloudFlare Spectrum",
      "TLS 1.3",
      "Let's Encrypt",
      "SASL",
      "WebSocket",
      "Go",
      "C++"
    ],
    "role": "architect",
    "innovations": [
      "Local LLM integration with Ollama for privacy-preserving AI chat capabilities",
      "Hierarchical hub-and-leaf network topology optimized for 1000-10,000 users",
      "Multi-platform bridge architecture unifying IRC, Discord, Slack, Telegram, and Matrix",
      "AI-specific security measures including prompt injection protection and rate limiting",
      "Defense-in-depth network segmentation with DMZ, protected internal, and isolated database layers"
    ],
    "enterprise_relevance": "Demonstrates enterprise architecture skills including high-availability design, load balancing with HAProxy, comprehensive monitoring with Prometheus/Grafana, DDoS mitigation strategies, TLS 1.3 security implementation, and database optimization for PostgreSQL at scale supporting 5,000-10,000 concurrent users",
    "maker_relevance": "Provides complete technical blueprint for self-hosted communication infrastructure with privacy-first AI integration, eliminating per-token costs and external data transmission while enabling full customization of models and parameters for technical communities",
    "category": "hybrid",
    "claude_involvement": "Research and architectural documentation synthesizing modern IRC ecosystem, bridge technologies, security best practices, and local LLM integration patterns for AI-centric deployments",
    "complexity_score": 9,
    "resume_highlight": "Architected enterprise-grade IRC infrastructure supporting 10,000+ users with local LLM integration, multi-platform bridging across 15+ services, and comprehensive security hardening including DDoS protection and TLS 1.3 implementation",
    "project_dir": "llamirc",
    "scanned_at": "2026-01-15T11:12:58.670899"
  },
  {
    "name": "MNIST OCR from Scratch in Common Lisp",
    "tagline": "Educational neural network implementation for digit recognition without ML libraries",
    "purpose": "Built a complete feedforward neural network from scratch in Common Lisp to recognize handwritten digits from the MNIST dataset. Implemented all core components including binary data loading, matrix operations, activation functions, backpropagation, and training loops without external ML libraries, demonstrating deep understanding of neural network fundamentals.",
    "technologies": [
      "Common Lisp",
      "SBCL",
      "Neural Networks",
      "Backpropagation",
      "MNIST"
    ],
    "role": "developer",
    "innovations": [
      "Pure Lisp implementation of neural networks without ML libraries",
      "SBCL-specific optimizations with type declarations for numerical performance",
      "Educational approach to implementing backpropagation from mathematical foundations"
    ],
    "enterprise_relevance": "Demonstrates deep understanding of machine learning fundamentals and ability to implement complex algorithms from scratch, valuable for custom ML solutions and debugging production systems",
    "maker_relevance": "Perfect educational project showing how to build neural networks from first principles, useful for learning ML fundamentals and functional programming approaches to numerical computing",
    "category": "adventure",
    "claude_involvement": null,
    "complexity_score": 7,
    "resume_highlight": "Implemented complete neural network training pipeline from scratch including IDX binary parsing, matrix operations, backpropagation, and SBCL performance optimizations",
    "project_dir": "llisp",
    "scanned_at": "2026-01-15T11:13:08.216238"
  },
  {
    "name": "Lochness Data Platform",
    "tagline": "Comprehensive data analytics platform for email marketing performance and contact ownership management",
    "purpose": "Lochness is a multi-schema data warehouse that consolidates email marketing events, contact ownership matrices, commission tracking, and fundraising data. It enables performance analysis, cost optimization, and revenue opportunity identification across 158M+ contacts and 14.5B+ email events.",
    "technologies": [
      "Snowflake",
      "SQL",
      "S3",
      "Airtable",
      "JSON",
      "Data Warehousing",
      "ETL Pipelines"
    ],
    "role": "architect",
    "innovations": [
      "Shared ownership matrix calculating fractional contact attribution (1/n) across multiple list owners",
      "Monster classification system for automated cost/revenue anomaly detection (KRAKEN, PHOENIX, etc.)",
      "Multi-platform quarantine system preventing global and client-specific opt-outs across email and P2P channels",
      "LIST_BUILDER pipeline transforming raw S3 JSON into normalized contact matrices"
    ],
    "enterprise_relevance": "Demonstrates large-scale data architecture for marketing operations with complex multi-tenant attribution, compliance management, and financial reconciliation across billions of records",
    "maker_relevance": null,
    "category": "professional",
    "claude_involvement": null,
    "complexity_score": 9,
    "resume_highlight": "Architected multi-schema data warehouse managing 158M contacts and 14.5B events with fractional ownership attribution and automated performance classification",
    "project_dir": "lochness",
    "scanned_at": "2026-01-15T11:13:18.707938"
  },
  {
    "name": "MyFinalWishes",
    "tagline": "Comprehensive digital estate planning and legacy management platform",
    "purpose": "MyFinalWishes enables users to organize important documents, track assets, designate beneficiaries, record final messages, and manage their digital legacy securely. The platform provides comprehensive estate planning tools with encrypted storage, trusted contact notifications, and subscription-based feature access.",
    "technologies": [
      "FastAPI",
      "React",
      "TypeScript",
      "PostgreSQL",
      "SQLModel",
      "TanStack Router",
      "TanStack Query",
      "Chakra UI",
      "Docker",
      "Nginx",
      "Alembic",
      "Pydantic",
      "Zod",
      "Vite"
    ],
    "role": "architect",
    "innovations": [
      "Encrypted sensitive data storage with field-level encryption",
      "Trusted contact notification system with role-based access",
      "Comprehensive digital legacy management with media recording capabilities",
      "Type-safe end-to-end development with generated schemas"
    ],
    "enterprise_relevance": "Demonstrates full-stack architecture for sensitive data management, subscription-based SaaS platform design, secure document management systems, and compliance-focused data protection patterns applicable to fintech and legal tech enterprises",
    "maker_relevance": "Complete estate planning solution addressing real user needs for digital legacy management, showcasing practical implementation of modern web stack with security best practices",
    "category": "professional",
    "claude_involvement": null,
    "complexity_score": 9,
    "resume_highlight": "Architected comprehensive estate planning platform with encrypted document management, multi-tier subscription system, and trusted contact notification framework serving sensitive user data with enterprise-grade security",
    "project_dir": "myfinalwishes",
    "scanned_at": "2026-01-15T11:13:29.044983"
  },
  {
    "name": "Clarion API",
    "tagline": "AI-powered newsletter aggregation and digest platform with creator tools",
    "purpose": "A comprehensive REST API platform that aggregates newsletters and RSS feeds, generates AI-powered daily digests, and provides a creator platform for publishing articles. The system handles user management, subscriptions, notifications, and content discovery with multi-provider AI integration.",
    "technologies": [
      "REST API",
      "JWT Authentication",
      "Ollama",
      "Claude AI",
      "Mistral OCR",
      "PostgreSQL",
      "Server-Sent Events",
      "Stripe",
      "PayPal",
      "WebSocket"
    ],
    "role": "architect",
    "innovations": [
      "Multi-provider AI orchestration (local Ollama + cloud Claude/Mistral)",
      "Streaming AI chat with SSE",
      "Social feed sharing workflow",
      "Hybrid creator platform within digest aggregator",
      "Premium AI mode switching"
    ],
    "enterprise_relevance": "Production-grade API with 85 endpoints, role-based access control, multi-tenant billing, webhook integrations, job queue system, and comprehensive audit trails suitable for SaaS deployment",
    "maker_relevance": "Demonstrates full-stack API design with AI integration, background job processing, real-time features, and monetization infrastructure",
    "category": "professional",
    "claude_involvement": "Claude AI integrated as premium cloud provider for high-quality summarization and topic extraction alongside local Ollama models",
    "complexity_score": 9,
    "resume_highlight": "Architected 85-endpoint REST API with multi-provider AI orchestration, streaming chat, creator platform, and subscription billing",
    "project_dir": "oym",
    "scanned_at": "2026-01-15T11:13:39.986077"
  },
  {
    "name": "DSM Platform",
    "tagline": "IPv6-native distributed system management platform with multicast service discovery",
    "purpose": "A distributed system management platform that enables task distribution and execution across a cluster of nodes using IPv6 networking, mDNS service discovery, and lock-free task queuing. The system implements work stealing for load balancing and includes comprehensive security features with Ed25519/Curve25519 cryptography.",
    "technologies": [
      "C",
      "IPv6",
      "mDNS",
      "multicast",
      "Ed25519",
      "Curve25519",
      "seccomp-bpf",
      "Python",
      "MkDocs",
      "Git",
      "systemd"
    ],
    "role": "architect",
    "innovations": [
      "Lock-free circular buffer task queue",
      "IPv6-native multicast service discovery",
      "Work stealing algorithm for distributed load balancing",
      "Sandboxed task execution with seccomp-bpf",
      "Automated deployment system with webhook integration"
    ],
    "enterprise_relevance": "Demonstrates distributed systems architecture, security implementation, automated deployment pipelines, and production-grade infrastructure management suitable for enterprise cluster computing",
    "maker_relevance": "Self-hosted distributed computing platform for Raspberry Pi clusters with comprehensive documentation and automated setup scripts",
    "category": "hybrid",
    "claude_involvement": null,
    "complexity_score": 9,
    "resume_highlight": "Architected IPv6-native distributed system with lock-free task queuing, mDNS service discovery, and cryptographic security across multi-node cluster",
    "project_dir": "plumbr",
    "scanned_at": "2026-01-15T11:13:49.310995"
  },
  {
    "name": "Bradley S. Isenbek Professional Portfolio",
    "tagline": "Software architect building production-scale systems and garage lab innovations",
    "purpose": "Comprehensive professional documentation showcasing 15+ years of software architecture experience across high-volume messaging platforms, data processing systems, and distributed infrastructure. Demonstrates both enterprise production expertise and maker mentality through personal projects involving hardware clusters, cryptographic systems, and custom networking protocols.",
    "technologies": [
      "Python",
      "Bash",
      "AWS",
      "Snowflake",
      "FastAPI",
      "DynamoDB",
      "PostgreSQL",
      "Redis",
      "ElasticSearch",
      "C",
      "Go",
      "OpenWRT",
      "ARM Assembly",
      "Docker",
      "Kubernetes",
      "Raspberry Pi",
      "Tor",
      "VPN",
      "Playwright",
      "Twilio",
      "Apache Solr",
      "Nginx",
      "Git"
    ],
    "role": "architect",
    "innovations": [
      "60-node Raspberry Pi distributed investigation cluster with VPN/Tor architecture",
      "Thorium-based TRNG system with statistical validation",
      "Custom 802.11 protocol for decentralized networking",
      "Vectl lightweight infrastructure management tool",
      "Dynamic Playwright code injection using trained models",
      "Atomic queue operations using low-level DynamoDB",
      "4.9 billion data point integration system"
    ],
    "enterprise_relevance": "Extensive production experience architecting high-availability messaging platforms processing millions of daily messages, data systems handling 100M+ contacts and billions of transactions, leading classified government projects, and managing technical teams at major companies like TransUnion",
    "maker_relevance": "Strong DIY ethos demonstrated through salvaged hardware projects, self-hosted infrastructure including custom DNS server, building tools from scratch when existing solutions are inadequate, and continuous experimentation with low-level systems programming",
    "category": "hybrid",
    "claude_involvement": "Integrated Claude as co-worker into development workflow for enhanced productivity beyond simple code completion",
    "complexity_score": 9,
    "resume_highlight": "Architected systems processing 4.9 billion data points while simultaneously building 60-node Raspberry Pi cluster for distributed investigation with custom VPN/Tor architecture",
    "project_dir": "resume",
    "scanned_at": "2026-01-15T11:14:01.263975"
  },
  {
    "name": "Seedbed Orchestrator",
    "tagline": "AI-driven knowledge synthesis platform with iterative web crawling and intelligent content integration",
    "purpose": "A sophisticated orchestration system that manages automated research workflows by coordinating web crawling, content evaluation, knowledge synthesis, and iterative gap analysis. The system uses multiple specialized components working together to gather comprehensive information on any topic, synthesize findings, and generate detailed research reports.",
    "technologies": [
      "Python",
      "Bash",
      "Node.js",
      "JSON",
      "Markdown",
      "Ollama",
      "LLM",
      "curl",
      "lynx",
      "jq"
    ],
    "role": "architect",
    "innovations": [
      "Iterative knowledge gap analysis with automated follow-up query generation",
      "Centralized JSON-based state management with atomic updates and file locking",
      "Multi-engine web crawling with HTML-to-Markdown conversion pipeline",
      "Coordinated multi-component workflow with graceful error handling and recovery"
    ],
    "enterprise_relevance": "Automated research and knowledge synthesis capabilities applicable to competitive intelligence, market research, technical documentation generation, and enterprise knowledge management systems",
    "maker_relevance": "Self-hosted AI research assistant that can autonomously gather and synthesize information on any topic using local LLMs, useful for personal research projects and learning",
    "category": "hybrid",
    "claude_involvement": "LLM integration for content synthesis, quality assessment, and final report generation using Ollama with configurable models",
    "complexity_score": 9,
    "resume_highlight": "Architected multi-component orchestration system with stateful workflow management, coordinating web crawling, AI-powered evaluation, and iterative knowledge synthesis",
    "project_dir": "seedbed",
    "scanned_at": "2026-01-15T11:14:12.681858"
  },
  {
    "name": "PROJECT SOVEREIGN",
    "tagline": "An assembly-like agentic programming language for self-improving systems",
    "purpose": "PROJECT SOVEREIGN is a revolutionary programming language that combines assembly-like simplicity with a minimal 32-instruction set, stack-based execution, and local LLM integration. It enables error-driven evolution for self-improvement and features homoiconic design for code-as-data manipulation, allowing programs to autonomously evolve and improve themselves.",
    "technologies": [
      "Python 3.13",
      "Lark Parser",
      "Ollama",
      "LLM",
      "Ruff",
      "Pyright",
      "uv",
      "pytest",
      "nox"
    ],
    "role": "architect",
    "innovations": [
      "32-instruction minimal assembly-like language with dual-stack architecture",
      "Error-driven evolution system enabling self-improving code",
      "Homoiconic design allowing programs to modify themselves",
      "Integrated local LLM code generation (LLMGEN and EVOLVE opcodes)",
      "Stack-based VM with built-in AI agent capabilities"
    ],
    "enterprise_relevance": "Demonstrates advanced compiler/interpreter design, AI-assisted code generation, and self-healing system architectures applicable to autonomous software maintenance and evolution",
    "maker_relevance": "Open-source programming language project showcasing language design, VM implementation, and practical LLM integration for code generation and self-improvement",
    "category": "hybrid",
    "claude_involvement": null,
    "complexity_score": 9,
    "resume_highlight": "Architected a complete programming language with 32-opcode instruction set, dual-stack VM, and integrated LLM-driven self-improvement capabilities",
    "project_dir": "sovereign",
    "scanned_at": "2026-01-15T11:14:23.113535"
  },
  {
    "name": "Spondr Aviation AI/ML Framework",
    "tagline": "Real-time aviation intelligence platform with standardized ML pipeline for live ADS-B aircraft data",
    "purpose": "A production-ready framework enabling rapid development and deployment of aviation AI models using live ADS-B data streams. Built with standardized templates and proven architecture, it supports multiple simultaneous models for aircraft tracking, landing prediction, anomaly detection, and flight path forecasting with sub-100ms latency.",
    "technologies": [
      "Python",
      "WebSocket",
      "DuckDB",
      "scikit-learn",
      "ADS-B",
      "Machine Learning",
      "Real-time Data Processing",
      "JSONL",
      "Node.js"
    ],
    "role": "architect",
    "innovations": [
      "Standardized ML pipeline for aviation data with <100ms prediction latency",
      "Multi-model framework processing 50+ aircraft/second simultaneously",
      "Template-based system enabling new model deployment in 5 steps",
      "Real-time feature engineering from live ADS-B streams with 94%+ accuracy"
    ],
    "enterprise_relevance": "Demonstrates production ML architecture, real-time data pipeline design, and scalable framework development applicable to IoT, logistics, and streaming analytics systems",
    "maker_relevance": "Aviation enthusiast project processing live aircraft telemetry with custom ML models for flight tracking and prediction",
    "category": "hybrid",
    "claude_involvement": null,
    "complexity_score": 9,
    "resume_highlight": "Architected real-time aviation ML framework processing 120+ aircraft/second with 94% prediction accuracy and <50ms latency, featuring 6 production models and standardized deployment pipeline",
    "project_dir": "spondr",
    "scanned_at": "2026-01-15T11:14:35.120095"
  },
  {
    "name": "Spondr Aviation AI/ML Framework",
    "tagline": "Real-time aviation intelligence platform with standardized ML pipeline for live ADS-B aircraft data",
    "purpose": "A production-ready framework for rapid development and deployment of aviation AI models using live ADS-B data streams. Enables multiple simultaneous models for aircraft behavior prediction, anomaly detection, and traffic optimization with sub-100ms latency and 50+ aircraft/second throughput.",
    "technologies": [
      "Python",
      "Machine Learning",
      "WebSocket",
      "ADS-B",
      "DuckDB",
      "scikit-learn",
      "Real-time Data Processing",
      "Node.js"
    ],
    "role": "architect",
    "innovations": [
      "Standardized ML pipeline for aviation data with <100ms prediction latency",
      "Multi-model architecture supporting simultaneous real-time inference from single data stream",
      "Template-based framework enabling new model deployment in 5 steps",
      "Production-ready landing predictor achieving 94%+ accuracy with live ADS-B integration"
    ],
    "enterprise_relevance": "Demonstrates scalable real-time ML architecture, production system design, and standardized development workflows applicable to IoT/telemetry processing, predictive maintenance, and operational intelligence platforms",
    "maker_relevance": "Aviation enthusiast project processing live aircraft data with accessible ML techniques, showcasing practical application of data science to real-world flight tracking and prediction",
    "category": "hybrid",
    "claude_involvement": null,
    "complexity_score": 9,
    "resume_highlight": "Architected production ML framework processing 50+ aircraft/second with 94% prediction accuracy and <100ms latency, featuring standardized pipeline enabling rapid model development",
    "project_dir": "spondr-2",
    "scanned_at": "2026-01-15T11:14:46.652580"
  },
  {
    "name": "WATR Protocol",
    "tagline": "Custom wireless protocol implementation with 802.11 packet injection and monitoring capabilities",
    "purpose": "A complete wireless protocol stack implementing custom packet framing, transmission, and reception over 802.11 networks. Provides both Python and C++ APIs for crafting and parsing protocol packets, with specialized support for monitor mode interfaces and packet injection on embedded Linux systems.",
    "technologies": [
      "Python",
      "C++",
      "Scapy",
      "802.11",
      "RadioTap",
      "ARM64",
      "Raspberry Pi",
      "Linux",
      "WiFi",
      "Bluetooth"
    ],
    "role": "architect",
    "innovations": [
      "Custom wireless protocol with LLC/SNAP encapsulation",
      "Cross-platform API (Python/C++) for packet manipulation",
      "Automated WiFi adapter detection and monitor mode setup",
      "802.11 data frame injection system"
    ],
    "enterprise_relevance": "Demonstrates low-level networking expertise, protocol design, and embedded systems development applicable to IoT, wireless infrastructure, and network security tools",
    "maker_relevance": "Enables DIY wireless communication projects on Raspberry Pi with custom protocols, useful for mesh networks, sensor networks, and experimental wireless systems",
    "category": "hybrid",
    "claude_involvement": null,
    "complexity_score": 8,
    "resume_highlight": "Architected custom wireless protocol with dual-language API supporting 802.11 packet injection and monitor mode on embedded ARM systems",
    "project_dir": "watr",
    "scanned_at": "2026-01-15T11:14:56.471940"
  },
  {
    "name": "Zephyr Handler Control API",
    "tagline": "REST and WebSocket API for remote management of hot-loading handlers in mesh networks",
    "purpose": "Provides comprehensive remote management capabilities for the Zephyr hot-loading handler system, enabling dynamic loading, unloading, monitoring, and discovery of handlers across mesh networks. Supports both REST endpoints and real-time WebSocket connections for handler lifecycle management and event streaming.",
    "technologies": [
      "FastAPI",
      "WebSocket",
      "JWT",
      "Python",
      "asyncio",
      "REST API"
    ],
    "role": "architect",
    "innovations": [
      "Hot-loading handler management via REST API",
      "Real-time handler event streaming over WebSocket",
      "Multi-source handler loading (file, code, upload)",
      "Distributed handler discovery across mesh network"
    ],
    "enterprise_relevance": "Enables centralized management and monitoring of distributed handler systems, supporting DevOps workflows with dynamic code deployment, real-time observability, and zero-downtime updates across mesh networks",
    "maker_relevance": "Provides accessible web-based interface for managing custom handlers on embedded devices, allowing rapid prototyping and iteration without device restarts or physical access",
    "category": "hybrid",
    "claude_involvement": null,
    "complexity_score": 8,
    "resume_highlight": "Designed comprehensive REST and WebSocket API architecture for distributed hot-loading system with authentication, real-time event streaming, and multi-source handler deployment",
    "project_dir": "zephyr",
    "scanned_at": "2026-01-15T11:15:07.822757"
  },
  {
    "name": "Zephyr",
    "tagline": "WiFi AI Mesh Network - distributed AI communication system using 802.11 frames",
    "purpose": "A distributed AI communication system that creates intelligent mesh networks with LLM integration using raw 802.11 and 802.3 frames. Enables peer-to-peer AI communication over WiFi and Ethernet with memory systems, priority-based request queuing, and streaming capabilities.",
    "technologies": [
      "Python",
      "802.11",
      "802.3",
      "Ethernet",
      "WiFi",
      "LLM",
      "Ollama",
      "Vector Embeddings",
      "Vectl",
      "Raw Sockets",
      "Async/Await"
    ],
    "role": "architect",
    "innovations": [
      "Raw 802.11 frame-based mesh networking for AI communication",
      "Distributed LLM processing with priority queuing and streaming",
      "Vector memory cells with clustering and evolution capabilities",
      "Custom EtherType protocol for AI mesh networks"
    ],
    "enterprise_relevance": "Demonstrates distributed systems architecture, network protocol design, and edge AI deployment patterns applicable to IoT, industrial networks, and decentralized AI systems",
    "maker_relevance": "Enables hobbyists to build AI mesh networks on Raspberry Pi and similar devices using commodity WiFi adapters, creating local-first AI communication without internet dependency",
    "category": "hybrid",
    "claude_involvement": "System integrates LLM services (Ollama) for distributed AI chat and query processing across mesh network nodes with configurable models and streaming responses",
    "complexity_score": 9,
    "resume_highlight": "Architected distributed AI mesh network using raw 802.11/Ethernet frames with integrated LLM processing, priority-based request queuing, and vector memory systems",
    "project_dir": "zephyr2",
    "scanned_at": "2026-01-15T11:15:20.078726"
  }
]